import subprocess
import numpy as np
import cv2
import datetime
from ultralytics import YOLO
import torch
from torchvision import transforms
import torch.nn as nn
import timm


# ë””ë²„ê·¸ ëª¨ë“œ (ë¹„ë””ì˜¤ ì¬ìƒ)
DEBUG_MODE = True


# ì„¤ì •ê°’
WIDTH, HEIGHT = 640, 640
FRAME_SKIP = 5  # 15fps ì¤‘ 3fpsë§Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ 5í”„ë ˆì„ë‹¹ 1íšŒ ì¶”ë¡ 
FRAME_SIZE = WIDTH * HEIGHT * 3
OFFSET = 9      # ì•½ 3ì´ˆ(3fps * 3s) ë™ì•ˆ ìì„¸ê°€ ìœ ì§€ë˜ì–´ì•¼ ë³€ê²½ìœ¼ë¡œ ì¸ì •
INF = -123456789
CONF_THRES = 0.5
IOU_THRES = 0.5


# ì¶”ë¡ ìš©: í¬ë¡­ëœ ì´ë¯¸ì§€ + (ë°”ìš´ë”© ë°•ìŠ¤ + í‚¤í¬ì¸íŠ¸)
def build_hybrid_inputs(image_bgr, bbox, bbox_n, kpts_tensor, device):
    # Crop person region
    crop = crop_image(image_bgr, bbox)
    if crop is None : return None, None
    img_tensor = crop.unsqueeze(0).to(device)

    kpts_flat = kpts_tensor.reshape(-1) # (51,)
    kpts_add = torch.cat([bbox_n, kpts_flat], dim=0) # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ê°€
    kpt_tensor = kpts_add.unsqueeze(0).float() # (1, 55)

    return img_tensor, kpt_tensor

# ì˜ˆì¸¡ ê²°ê³¼ í‚¤í¬ì¸íŠ¸ ì²˜ë¦¬
def predict_with_distinction(model, img, kpts, device):
    model.eval()
    with torch.no_grad():
        logits = model(img.to(device), kpts.to(device))
        probs = torch.softmax(logits, dim=1)[0]
        pred = int(torch.argmax(probs))
    
    # if probs[pred] < CONF_THRES:
    #     pred = rule_based_postprocess(kpts)
        
    return pred # ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

# ë£° ê¸°ë°˜ ê²°ê³¼ ë³´ì •
def rule_based_postprocess(kpts_tensor):

    kpts = kpts_tensor.detach().cpu().numpy().flatten()

    if kpts.size != 55:
        return 4
     
    # kpts 55ê°œ
    kpts = kpts[4:]
    kpts = kpts.reshape(17,3)

    nose = kpts[0]
    l_shoulder, r_shoulder = kpts[5], kpts[6]
    l_wrist, r_wrist = kpts[9], kpts[10]
    l_hip, r_hip = kpts[11], kpts[12]

    shoulder_width = abs(l_shoulder[0] - r_shoulder[0])
    torso_vec = np.array([(l_hip[0]+r_hip[0])/2 - (l_shoulder[0]+r_shoulder[0])/2,
                          (l_hip[1]+r_hip[1])/2 - (l_shoulder[1]+r_shoulder[1])/2])
    torso_len = np.linalg.norm(torso_vec)
    torso_angle = np.arctan2(torso_vec[1], torso_vec[0]) * 180 / np.pi

    # -----------------------------
    # 1) Lying ë¨¼ì € ì²´í¬ (ë°”ë¡œ ëˆ„ì›€)
    # torso ê±°ì˜ ìˆ˜í‰, shoulder ë„“ìŒ, ì†ëª©ì´ ì–¼êµ´ ìœ„ê°€ ì•„ë‹Œ ê²½ìš°
    if abs(torso_angle) < 20 and shoulder_width > torso_len * 0.5 and \
       not ((l_wrist[2] > CONF_THRES and l_wrist[1] < l_shoulder[1]) or \
            (r_wrist[2] > CONF_THRES and r_wrist[1] < r_shoulder[1])):
        return 0  # lying

    # -----------------------------
    # 2) Hand-up
    if (l_wrist[2] > CONF_THRES and l_wrist[1] < l_shoulder[1]) or \
       (r_wrist[2] > CONF_THRES and r_wrist[1] < r_shoulder[1]):
        return 2  # handup

    # -----------------------------
    # 3) Back (ì—ë“œë¦¼)
    if nose[2] < CONF_THRES and (l_shoulder[2] > CONF_THRES or r_shoulder[2] > CONF_THRES):
        return 3  # back

    # -----------------------------
    # 4) Side
    if 45 < abs(torso_angle) < 135 and shoulder_width < torso_len * 0.7:
        return 1  # side

    # -----------------------------
    # 5) ê¸°íƒ€
    return 4

# ì´ë¯¸ì§€ í¬ë¡­
def crop_image(img, bbox):
    if img is None:
        return None
    
    x1, y1, x2, y2 = bbox
    h, w = img.shape[:2]
    x1, y1 = max(0, x1), max(0, y1)
    x2, y2 = min(w, x2), min(h, y2)

    if x2 <= x1 or y2 <= y1:
        return None

    crop = img[y1:y2, x1:x2]
    crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)

    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    return transform(crop_rgb)

# CNN
class ImageEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)
        self.model = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=True)
        self.out_dim = 1280
        
        # ê°€ì¤‘ì¹˜ ë™ê²°
        for param in self.model.parameters():
            param.requires_grad = False

    def forward(self, x):
        x = self.model.forward_features(x) # (Batch, 1280, 7, 7) í˜•íƒœ
        x = torch.mean(x, dim=(2, 3), keepdim=True)
        return x.flatten(1)

# MLP
class KeypointEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(4 + 17 * 3, 128),
            nn.BatchNorm1d(128), # í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ ì¶”ê°€ ê¶Œì¥
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU()
        )
        self.out_dim = 256

    def forward(self, kpts):
        return self.net(kpts.flatten(1))

# ëª¨ë¸ ë³¸ì²´
class SleepPoseNet(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.img_enc = ImageEncoder()
        self.kpt_enc = KeypointEncoder()

        self.classifier = nn.Sequential(
            nn.Linear(self.img_enc.out_dim + self.kpt_enc.out_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, img, kpts):
        f_img = self.img_enc(img)
        f_kpt = self.kpt_enc(kpts)
        return self.classifier(torch.cat([f_img, f_kpt], dim=1))



device = 'cuda' if torch.cuda.is_available() else 'cpu'
hybrid_weights = r"C:\Users\USER\Documents\Github\SleepPose\Inference_Server\pose_pt\pose_4_22e_rl1e-4_best\sleep_pose_best_model.pt"

# ===== ì¶”ë¡  ëª¨ë¸ ë¡œë“œ =====
hybrid_model = SleepPoseNet(num_classes=5).to(device)
hybrid_model.load_state_dict(torch.load(hybrid_weights, map_location=device))
hybrid_model.eval()

# ===== YOLO ëª¨ë¸ ë¡œë“œ =====
yolo_model = YOLO("yolo11n-pose.pt")

def save_to_mariadb(user_id, sleep_data_list):
    """
    ë§ˆë¦¬ì•„ë””ë¹„ ì €ì¥ ë©”ì„œë“œ (ë‚´ìš©ì€ ë‚˜ì¤‘ì— ì±„ì›€)
    sleep_data_list: [{'pose': 'ìì„¸ëª…', 'start': 'ì‹œê°„', 'end': 'ì‹œê°„'}, ...]
    """
    print(f"\nğŸ’¾ [DB ì €ì¥] ìœ ì € {user_id}ì˜ ìˆ˜ë©´ ê¸°ë¡ {len(sleep_data_list)}ê±´ ì €ì¥ ì‹œë„ ì¤‘...")
    # SQL ì—°ê²° ë° INSERT ë¡œì§ì´ ë“¤ì–´ê°ˆ ìë¦¬
    for data in sleep_data_list:
        print(f" > {data['pose']}: {data['start']} ~ {data['end']}")

def run_ffmpeg_yolo(rtsp_url: str, ffmpeg_path: str, stop_flag: callable, user_id: int):

    if DEBUG_MODE:
        cap = cv2.VideoCapture(r"C:\Users\USER\Documents\Github\SleepPose\Inference_Server\data\lee_video\infer_Oh.mp4")
    else:
        cmd = [
            ffmpeg_path, "-rtsp_transport", "tcp", "-fflags", "nobuffer",
            "-flags", "low_delay", "-i", rtsp_url,
            "-vf", f"scale={WIDTH}:{HEIGHT}", "-pix_fmt", "bgr24",
            "-f", "rawvideo", "-"
        ]

        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, bufsize=10**8)

    # --- ìì„¸ ê¸°ë¡ìš© ë³€ìˆ˜ ---
    sleep_timeline = []  # ìµœì¢… DBë¡œ ë³´ë‚¼ ë¦¬ìŠ¤íŠ¸
    
    current_pose = INF
    start_time = datetime.datetime.now()
    
    pending_pose = None  # ìƒˆë¡œ ë°”ë€ ê²ƒì²˜ëŸ¼ ë³´ì´ëŠ” ìì„¸
    pending_start_time = None
    consistent_count = 0  # í•´ë‹¹ ìì„¸ê°€ ëª‡ ë²ˆ ì§€ì†ë˜ì—ˆëŠ”ì§€ ì¹´ìš´íŠ¸

    frame_count = 0
    print("âœ… FFmpeg YOLO ìŠ¤íŠ¸ë¦¼ ë° íƒ€ì„ë¼ì¸ ë¶„ì„ ì‹œì‘")

    # COCO skeleton connections (ë””ë²„ê¹…ìš©)
    skeleton = [
        (5, 7), (7, 9),
        (6, 8), (8, 10),
        (5, 6),
        (5, 11), (6, 12),
        (11, 12),
        (11, 13), (13, 15),
        (12, 14), (14, 16)
    ]

    detected_pose = 4 # ê¸°ë³¸ê°’ (others)
    try:
        while not stop_flag():
            if DEBUG_MODE:
                ret, frame = cap.read()
                if not ret or frame is None: 
                    print("ğŸ›‘ DEBUG video ended") 
                    break
            else:
                raw_frame = process.stdout.read(FRAME_SIZE)
                if len(raw_frame) != FRAME_SIZE: break
                frame = np.frombuffer(raw_frame, dtype=np.uint8).reshape((HEIGHT, WIDTH, 3)).copy()

            frame_count += 1
            # 15fps ì¤‘ 3fps ì¶”ë¡  (5í”„ë ˆì„ë§ˆë‹¤ 1ë²ˆ)
            if frame_count % FRAME_SKIP != 0:
                continue

            now = datetime.datetime.now()

            # ===== YOLO ì¶”ë¡  =====
            results = yolo_model(frame, imgsz=640, device=0, half=True, verbose=False, conf=CONF_THRES, iou=IOU_THRES)
            result = results[0]

            # 1. ìì„¸ ê²°ì • (ì‚¬ëŒ ìœ ë¬´ì— ë”°ë¼)
            if len(result.boxes) > 0 and result.keypoints is not None:
                # [ì‚¬ëŒì´ ìˆì„ ë•Œ] ê¸°ì¡´ GPU ìµœì í™” ë¡œì§ ê·¸ëŒ€ë¡œ ìˆ˜í–‰
                bbox_xyxy = result.boxes.xyxy[0]
                x1, y1, x2, y2 = bbox_xyxy.int().tolist()
                bbox_pixel = (x1, y1, x2, y2)
                bbox_norm = result.boxes.xyxyn[0]
                kpts_norm = result.keypoints.xyn[0]
                kpts_conf = result.keypoints.conf[0].unsqueeze(1)
                kpts_n = torch.cat([kpts_norm, kpts_conf], dim=1)

                img_t, kpt_t = build_hybrid_inputs(frame, bbox_pixel, bbox_norm, kpts_n, device)
                if img_t is None or kpt_t is None: continue
                detected_pose = predict_with_distinction(hybrid_model, img_t, kpt_t, device)
            else:
                # [ì‚¬ëŒì´ ì—†ì„ ë•Œ] ê°•ì œë¡œ Others(4) ì²˜ë¦¬
                detected_pose = 4

            # 2. ìì„¸ ë³€í™” ë¡œì§ (Offset ê²€ì¦) 
            if detected_pose != current_pose:
                if detected_pose == pending_pose:
                    consistent_count += 1
                else:
                    pending_pose = detected_pose
                    pending_start_time = now
                    consistent_count = 1
                
                if consistent_count >= OFFSET:
                    if current_pose != INF:
                        sleep_timeline.append({
                            'pose': current_pose,
                            'start': start_time.strftime('%Y-%m-%d %H:%M:%S'),
                            'end': pending_start_time.strftime('%Y-%m-%d %H:%M:%S')
                        })
                    current_pose = pending_pose
                    start_time = pending_start_time
                    consistent_count = 0
                    pending_pose = None
            else:
                consistent_count = 0
                pending_pose = None

    finally:
        
        # ë°˜ë³µë¬¸ ì¢…ë£Œ ì‹œ ë§ˆì§€ë§‰ ìì„¸ ì €ì¥
        if current_pose != INF:
            sleep_timeline.append({
                'pose': current_pose,
                'start': start_time.strftime('%Y-%m-%d %H:%M:%S'),
                'end': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        if stop_flag() and not DEBUG_MODE:
            process.terminate()
        cv2.destroyAllWindows() # (ë””ë²„ê¹…ìš©)
        
        # ì°¨ê³¡ì°¨ê³¡ ìŒ“ì¸ ë°ì´í„°ë¥¼ DBë¡œ ì „ì†¡
        if sleep_timeline:
            save_to_mariadb(user_id, sleep_timeline)
        
        print("ğŸ›‘ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")


