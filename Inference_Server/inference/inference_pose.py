import subprocess
import numpy as np
import cv2
from datetime import datetime
from ultralytics import YOLO
import torch
from torchvision import transforms
import torch.nn as nn
import timm
from Inference_Server.inference.db_utils import get_db_connection

# ë””ë²„ê·¸ ëª¨ë“œ (True : ë¹„ë””ì˜¤ ì¬ìƒ / False : RTSP)
DEBUG_MODE = True

# ì„¤ì •ê°’
WIDTH, HEIGHT = 640, 640
FRAME_SKIP = 5  # ì´ 15fps ì¤‘ 3fpsë§Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ 5í”„ë ˆì„ë‹¹ 1íšŒ ì¶”ë¡ 
FRAME_SIZE = WIDTH * HEIGHT * 3
OFFSET = 18  # ì•½ 6ì´ˆ(3fps * 3s) ë™ì•ˆ ìì„¸ê°€ ìœ ì§€ë˜ì–´ì•¼ ë³€ê²½ìœ¼ë¡œ ì¸ì •
INF = -123456789 # Pose_id ì´ˆê¸°ê°’
CONF_THRES = 0.7 # í‚¤í¬ì¸íŠ¸ ì‹ ë¢°ë„ ê¸°ì¤€
IOU_THRES = 0.5 # yoloìš© iou ê¸°ì¤€
KPT_ALPHA = 0.85 # í‚¤í¬ì¸íŠ¸ ìŠ¤ë¬´ìŠ¤ ì´ë™ì„ ìœ„í•œ ì¡°ì •ê°’

# =========================================================
# Utils
# =========================================================

# ì¶”ë¡ ìš©: í¬ë¡­ëœ ì´ë¯¸ì§€ + (ë°”ìš´ë”© ë°•ìŠ¤ + í‚¤í¬ì¸íŠ¸)
def build_hybrid_inputs(image_bgr, bbox, bbox_n, kpts_tensor, device):
    # ë°”ìš´ë”©ë°•ìŠ¤ ê¸°ì¤€ ì´ë¯¸ì§€ í¬ë¡­ (ì‚¬ëŒë§Œ ë³´ì´ê²Œ)
    crop = crop_image(image_bgr, bbox)
    if crop is None : return None, None
    img_tensor = crop.unsqueeze(0).to(device)

    # ë°”ìš´ë”© ë°•ìŠ¤ + í‚¤í¬ì¸íŠ¸(ì •ê·œí™” ë¨)
    kpts_flat = kpts_tensor.reshape(-1) # (51,)
    kpts_add = torch.cat([bbox_n, kpts_flat], dim=0) # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ê°€
    kpt_tensor = kpts_add.unsqueeze(0).float() # (1, 55)

    return img_tensor, kpt_tensor

# ì˜ˆì¸¡ ê²°ê³¼ í‚¤í¬ì¸íŠ¸ ì²˜ë¦¬
def predict_with_distinction(model, img, kpts, device):
    model.eval()
    with torch.no_grad():
        logits = model(img.to(device), kpts.to(device))
        probs = torch.softmax(logits, dim=1)[0]
        pred = int(torch.argmax(probs))
    
    # ì¼ì • ë¯¸ë§Œ ì‹ ë¢°ë„ì¼ ì‹œ ë£° ê¸°ë°˜ ê²°ê³¼ ë³´ì • ì²˜ë¦¬
    if probs[pred] < CONF_THRES:
        pred = rule_based_postprocess(kpts)
        
    return pred # ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

# ë£° ê¸°ë°˜ ê²°ê³¼ ë³´ì •
def rule_based_postprocess(kpts, conf_thres=0.4, shoulder_parallel_deg=20):
    if isinstance(kpts, torch.Tensor):
        kpts = kpts.detach().cpu()

    # batch ì°¨ì› ì œê±°
    if kpts.ndim == 2:
        kpts = kpts[0]

    if kpts.numel() != 55:
        print(f"[DEBUG] invalid kpts numel = {kpts.numel()}")
        return 4

    kpts = kpts.numpy()

    # bbox ì œê±°
    kpts = kpts[4:].reshape(17, 3)

    # ì£¼ìš” í‚¤í¬ì¸íŠ¸
    nose = kpts[0]
    l_eye, r_eye = kpts[1], kpts[2]
    l_shoulder, r_shoulder = kpts[5], kpts[6]
    l_wrist, r_wrist = kpts[9], kpts[10]

    # =========================
    # STEP 1. ì•/ë’¤ íŒë³„
    # =========================
    face_conf_cnt = sum([
        nose[2] > conf_thres,
        l_eye[2] > conf_thres,
        r_eye[2] > conf_thres
    ])

    is_front = face_conf_cnt >= 2

    # =========================
    # STEP 2. ì•ì„ ë³´ê³  ìˆëŠ” ê²½ìš°
    # =========================
    if is_front:
        # ì–´ê¹¨ì„  ê¸°ìš¸ê¸°
        dx = r_shoulder[0] - l_shoulder[0]
        dy = r_shoulder[1] - l_shoulder[1]
        shoulder_angle = np.degrees(np.arctan2(dy, dx))

        is_parallel = abs(shoulder_angle) < shoulder_parallel_deg

        # ì†ëª© ìœ„ì¹˜
        wrist_up = (
            (l_wrist[2] > conf_thres and l_wrist[1] < l_shoulder[1]) or
            (r_wrist[2] > conf_thres and r_wrist[1] < r_shoulder[1])
        )

        if is_parallel:
            return 2 if wrist_up else 0
        else:
            return 1  # ì˜†ìœ¼ë¡œ ëˆ„ì›€

    # =========================
    # STEP 3. ì–¼êµ´ ì•ˆ ë³´ì„ â†’ ì—ë“œë¦¼
    # =========================
    return 3

# ì´ë¯¸ì§€ í¬ë¡­
def crop_image(img, bbox):
    if img is None:
        return None
    
    x1, y1, x2, y2 = bbox
    h, w = img.shape[:2]
    x1, y1 = max(0, x1), max(0, y1)
    x2, y2 = min(w, x2), min(h, y2)

    if x2 <= x1 or y2 <= y1:
        return None

    crop = img[y1:y2, x1:x2]
    crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)

    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    return transform(crop_rgb)

# ì ì™¸ì„  í™˜ê²½ ë” ì˜ ë³´ì´ê²Œ ì²˜ë¦¬
def ir_preprocess(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(2.0, (8,8))
    gray = clahe.apply(gray)
    return cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)

# ìŠ¤ë¬´ìŠ¤í•˜ê²Œ í‚¤í¬ì¸íŠ¸ ì´ë™
def ema(prev, curr, alpha):
    return curr if prev is None else alpha * prev + (1 - alpha) * curr

# ê¸‰ê²©í•œ ì¢Œìš° ë’¤ì§‘í˜ ë°©ì§€
def enforce_lr_consistency(kpts):
    """ì¢Œ/ìš° ìŠ¤ì™‘ ë°©ì§€"""
    pairs = [(5,6),(7,8),(9,10),(11,12),(13,14),(15,16)]
    if torch.isnan(kpts[5]).any() or torch.isnan(kpts[6]).any():
        return kpts
    # ì–‘ ì–´ê¹¨ê°€ ìœ„ì¹˜ ë’¤ë°”ë€Œë©´ ì•„ì˜ˆ í‚¤ë¥¼ ë°˜ëŒ€ë¡œ ë’¤ì§‘ì–´ ì •ìƒí™” ì‹œí‚´
    if kpts[5,0] > kpts[6,0]:
        for a,b in pairs:
            kpts[[a,b]] = kpts[[b,a]]
    return kpts

# =========================================================
# Model
# =========================================================

# CNN
# ì‚¬ìš© ëª¨ë¸ tf_efficientnetv2_s.in21k_ft_in1k
# ì‚¬ì „í•™ìŠµ imageNet 21k, íŒŒì¸íŠœë‹ imageNet 1k, í•™ìŠµ ì‹œ - 2ì°¨ íŒŒì¸íŠœë‹: top ë ˆì´ì–´ë§Œ í•™ìŠµ
class ImageEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)
        self.model = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=True)
        self.out_dim = 1280
        
        # ê°€ì¤‘ì¹˜ ë™ê²°
        for param in self.model.parameters():
            param.requires_grad = False

        # top ë ˆì´ì–´ë§Œ í•™ìŠµ
        for name, param in self.model.named_parameters():
            if "blocks.4" in name or "blocks.5" in name: # ëë¶€ë¶„ë§Œ ë™ê²° í’€ì–´ì„œ í•™ìŠµì‹œí‚´ (ì¶”ë¡ ì—ì„  eval ëª¨ë“œë¼ í•™ìŠµ ì•ˆë¨)
                param.requires_grad = True

    def forward(self, x):
        x = self.model.forward_features(x) # (Batch, 1280, 7, 7) í˜•íƒœ
        x = torch.mean(x, dim=(2, 3), keepdim=True)
        return x.flatten(1)

# MLP
# (128) -> (256) -> (512)
class KeypointEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(4 + 17 * 3, 128),
            nn.BatchNorm1d(128), # í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ ì¶”ê°€ ê¶Œì¥
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU()
        )
        self.out_dim = 512

    def forward(self, kpts):
        return self.net(kpts.flatten(1))

# ëª¨ë¸ ë³¸ì²´
# CNN(1280) + MLP(512) -> 5
class SleepPoseNet(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.img_enc = ImageEncoder()
        self.kpt_enc = KeypointEncoder()

        self.classifier = nn.Sequential(
            nn.Linear(self.img_enc.out_dim + self.kpt_enc.out_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, img, kpts):
        f_img = self.img_enc(img)
        f_kpt = self.kpt_enc(kpts)
        return self.classifier(torch.cat([f_img, f_kpt], dim=1))

device = 'cuda' if torch.cuda.is_available() else 'cpu'
hybrid_weights = r"./pose_pt/pose_9_22e_rl1e-4_best/sleep_pose_best_model.pt"

# ===== ì¶”ë¡  ëª¨ë¸ ë¡œë“œ =====
hybrid_model = SleepPoseNet(num_classes=5).to(device)
hybrid_model.load_state_dict(torch.load(hybrid_weights, map_location=device))
hybrid_model.eval()

# ===== YOLO ëª¨ë¸ ë¡œë“œ =====
yolo_model = YOLO("yolo11n-pose.pt")


# =========================================================
# Main Method
# =========================================================

# ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹œ ì •ë³´ DB ì €ì¥
def save_to_mariadb(login_id, sleep_data_list):
    if not sleep_data_list:
        print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nğŸ’¾ [DB ì €ì¥] ìœ ì € {login_id} ìˆ˜ë©´ ê¸°ë¡ {len(sleep_data_list)}ê±´ ì €ì¥ ì‹œì‘")

    print("\nğŸ“‹ [ì €ì¥ ì˜ˆì • ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°]")
    for i, data in enumerate(sleep_data_list, 1):
        print(
            f"{i:02d}. "
            f"user_id={login_id}, "
            f"pose={data['pose']}, "
            f"start={data['start']}, "
            f"end={data['end']}"
        )


    # 1ï¸âƒ£ DB ì—°ê²°
    conn = get_db_connection()

    if conn is None:
        print("âŒ DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì €ì¥ ì¤‘ë‹¨")
        return

    try:
        with conn.cursor() as cur:
            insert_sql = """
            INSERT INTO sleep_pose2 (user_id, pose_class, st_dt, ed_dt, dt)
            VALUES (%s, %s, %s, %s, %s)
            """

            rows = []
            for data in sleep_data_list:
                rows.append((
                    login_id,
                    data['pose'],
                    datetime.fromisoformat(data['start']),
                    datetime.fromisoformat(data['end']),
                    datetime.now()
                ))

            # 2ï¸âƒ£ í•œ ë²ˆì— INSERT
            cur.executemany(insert_sql, rows)
            conn.commit()

            print(f"âœ… DB ì €ì¥ ì™„ë£Œ ({len(rows)}ê±´)")

    except Exception as e:
        conn.rollback()
        print("âŒ DB ì €ì¥ ì‹¤íŒ¨:", e)

    finally:
        conn.close()

# RTSP ì‹¤í–‰ ë° 
def run_ffmpeg_yolo(rtsp_url: str, ffmpeg_path: str, stop_flag: callable, login_id: int):

    # DEBUG_MODEì¼ ì‹œ ë¹„ë””ì˜¤ ì¶”ë¡ 
    if DEBUG_MODE:
        cap = cv2.VideoCapture(r".\data\lee_video\infer_Lee.mp4")
    else:
        cmd = [
            ffmpeg_path, "-rtsp_transport", "tcp", "-fflags", "nobuffer",
            "-flags", "low_delay", "-i", rtsp_url,
            "-vf", f"scale={WIDTH}:{HEIGHT}", "-pix_fmt", "bgr24",
            "-f", "rawvideo", "-"
        ]

        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, bufsize=10**8)

    # --- ìì„¸ ê¸°ë¡ìš© ë³€ìˆ˜ ---
    sleep_timeline = []  # ìµœì¢… DBë¡œ ë³´ë‚¼ ë¦¬ìŠ¤íŠ¸
    
    current_pose = INF
    start_time = datetime.now()
    
    pending_pose = None  # ìƒˆë¡œ ë°”ë€ ê²ƒì²˜ëŸ¼ ë³´ì´ëŠ” ìì„¸
    pending_start_time = None
    consistent_count = 0  # í•´ë‹¹ ìì„¸ê°€ ëª‡ ë²ˆ ì§€ì†ë˜ì—ˆëŠ”ì§€ ì¹´ìš´íŠ¸
    prev_kpts_norm = None
    frame_count = 0
    print("âœ… FFmpeg YOLO ìŠ¤íŠ¸ë¦¼ ë° íƒ€ì„ë¼ì¸ ë¶„ì„ ì‹œì‘")

    # COCO skeleton connections (ë””ë²„ê¹…ìš©)
    skeleton = [
        (5, 7), (7, 9),
        (6, 8), (8, 10),
        (5, 6),
        (5, 11), (6, 12),
        (11, 12),
        (11, 13), (13, 15),
        (12, 14), (14, 16)
    ]

    detected_pose = 4 # ê¸°ë³¸ê°’ (others)

    try:
        # stop_flag()ì´ ëŒë‹¤ í•¨ìˆ˜ê°€ boolê°’ì´ ë°”ë€ŒëŠ” ê²ƒì„ ê°ì§€, inference_running = False ë˜ê¸° ì´ì „ê¹Œì§€ ë°˜ë³µë¬¸ì´ ì‹¤í–‰
        # DEBUG_MODEì¼ ë• ì˜ìƒì´ ì¢…ë£Œë˜ì—ˆì„ ë•Œ ìë™ ì¢…ë£Œ
        while not stop_flag():
            if DEBUG_MODE:
                ret, frame = cap.read()
                if not ret or frame is None: 
                    print("ğŸ›‘ DEBUG video ended") 
                    break
            else:
                raw_frame = process.stdout.read(FRAME_SIZE)
                if len(raw_frame) != FRAME_SIZE: break
                frame = np.frombuffer(raw_frame, dtype=np.uint8).reshape((HEIGHT, WIDTH, 3)).copy()

            frame = ir_preprocess(frame) # ì ì™¸ì„  í™˜ê²½ ì²˜ë¦¬

            frame_count += 1
            # 15fps ì¤‘ 3fps ì¶”ë¡  (5í”„ë ˆì„ë§ˆë‹¤ 1ë²ˆ)
            if frame_count % FRAME_SKIP != 0:
                continue

            now = datetime.now()

            # ===== YOLO ì¶”ë¡  =====
            results = yolo_model(frame, imgsz=640, device=0, half=True, verbose=False, conf=CONF_THRES, iou=IOU_THRES)
            result = results[0]

            # 1. ìì„¸ ê²°ì • (ì‚¬ëŒ ìœ ë¬´ì— ë”°ë¼)
            if len(result.boxes) > 0 and result.keypoints is not None:
                bbox_xyxy = result.boxes.xyxy[0]
                x1, y1, x2, y2 = bbox_xyxy.int().tolist()
                bbox_pixel = (x1, y1, x2, y2) # ì›ë³¸ í”½ì…€ ê¸°ì¤€ ë°”ìš´ë”©ë°•ìŠ¤ (ì´ë¯¸ì§€ í¬ë¡­ì— í•„ìš”)
                bbox_norm = result.boxes.xyxyn[0] # ì •ê·œí™”ëœ ë°”ìš´ë”©ë°•ìŠ¤ (MLP ë°ì´í„°ì— í•„ìš”)

                kpts_norm = result.keypoints.xyn[0] # ì •ê·œí™”ëœ í‚¤í¬ì¸íŠ¸ (17, 2) (MLP ë°ì´í„°ì— í•„ìš”)
                kpts_norm = enforce_lr_consistency(kpts_norm) # ê¸‰ê²©í•œ ë’¤ì§‘í˜ ë°©ì§€
                kpts_norm = ema(prev_kpts_norm, kpts_norm, KPT_ALPHA) # í‚¤í¬ì¸íŠ¸ ìŠ¤ë¬´ìŠ¤ ì´ë™
                prev_kpts_norm = kpts_norm.clone()
                kpts_conf = result.keypoints.conf[0].unsqueeze(1) # í‚¤í¬ì¸íŠ¸ ì‹ ë¢°ë„ (17, 1)
                kpts_n = torch.cat([kpts_norm, kpts_conf], dim=1) # ì •ê·œí™”ëœ í‚¤í¬ì¸íŠ¸ + ì‹ ë¢°ë„ (17, 3)

                # ì´ë¯¸ì§€ í¬ë¡­, MLPìš© ë°ì´í„° ìƒì„±
                img_t, kpt_t = build_hybrid_inputs(frame, bbox_pixel, bbox_norm, kpts_n, device)
                if img_t is None or kpt_t is None: continue
                # ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë¸ì— ì¶”ë¡ 
                detected_pose = predict_with_distinction(hybrid_model, img_t, kpt_t, device)
            else:
                # [ì‚¬ëŒì´ ì—†ì„ ë•Œ] ê°•ì œë¡œ Others(4) ì²˜ë¦¬
                detected_pose = 4

            # 2. ìì„¸ ë³€í™” ë¡œì§ (Offset ê²€ì¦) 
            if detected_pose != current_pose:
                if detected_pose == pending_pose:
                    consistent_count += 1
                else:
                    pending_pose = detected_pose
                    pending_start_time = now
                    consistent_count = 1
                
                # ì§€ì •í•œ OFFSET ì´ìƒ ìì„¸ê°€ ìœ ì§€ë˜ì–´ì•¼ ì´ì „ ìì„¸ì˜ ë°ì´í„° ê¸°ë¡
                if consistent_count >= OFFSET:
                    if current_pose != INF:
                        sleep_timeline.append({
                            'pose': str(current_pose),
                            'start': start_time.strftime('%Y-%m-%d %H:%M:%S'),
                            'end': pending_start_time.strftime('%Y-%m-%d %H:%M:%S')
                        })
                    current_pose = pending_pose
                    start_time = pending_start_time
                    consistent_count = 0
                    pending_pose = None
            else:
                consistent_count = 0
                pending_pose = None

    finally:
        # ë°˜ë³µë¬¸ ì¢…ë£Œ ì‹œ ë§ˆì§€ë§‰ ìì„¸ ì €ì¥
        if current_pose != INF:
            sleep_timeline.append({
                'pose': str(current_pose),
                'start': start_time.strftime('%Y-%m-%d %H:%M:%S'),
                'end': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })

        if stop_flag() and not DEBUG_MODE:
            process.terminate()
        cv2.destroyAllWindows() # (ë””ë²„ê¹…ìš©)
        
        # ì°¨ê³¡ì°¨ê³¡ ìŒ“ì¸ ë°ì´í„°ë¥¼ DBë¡œ ì „ì†¡
        if sleep_timeline:
            save_to_mariadb(login_id, sleep_timeline)
        
        print("ğŸ›‘ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")


