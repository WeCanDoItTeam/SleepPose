import sys
import os
import torch
import torchaudio.transforms as T
import numpy as np
import subprocess
import wave
from datetime import datetime, timedelta
from Inference_Server.inference.db_utils import get_db_connection

# --- ì„¤ì •ê°’ ---
SAMPLE_RATE = 16000
CHUNK_DURATION = 1  # 1ì´ˆ
CHUNK_SIZE = SAMPLE_RATE * CHUNK_DURATION * 2  # 16bit = 2bytes -> 32000 bytes
CONF_THRESHOLD = 0.6  # ì‹œì‘ ì„ê³„ê°’ (60%)
SILENCE_TIMEOUT = 10  # ë‹¤ë¥¸ ì´ë²¤íŠ¸ë‚˜ ì†ŒìŒì´ ì§€ì†ë˜ë©´ ì¢…ë£Œí•  ì‹œê°„ (ì´ˆ)
RECORDING_DIR = "./recordings"  # ë…¹ìŒ íŒŒì¼ ì €ì¥ ê²½ë¡œ

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(RECORDING_DIR, exist_ok=True)

# --- 1. ì „ì²˜ë¦¬ ë° ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼) ---
def rms_normalize(audio_chunk, target_rms=0.1):
    rms = np.sqrt(np.mean(audio_chunk**2))
    if rms < 1e-6:
        return audio_chunk
    gain = target_rms / rms
    return audio_chunk * gain

def preprocess_audio_chunk(audio_chunk):
    audio_chunk = rms_normalize(audio_chunk)
    if isinstance(audio_chunk, np.ndarray):
        audio_chunk = torch.from_numpy(audio_chunk).float()
    if len(audio_chunk.shape) == 1:
        audio_chunk = audio_chunk.unsqueeze(0)

    transform = T.MelSpectrogram(
        sample_rate=SAMPLE_RATE, n_fft=512, win_length=400, hop_length=160,
        n_mels=64, f_min=125, f_max=7500, center=False
    )
    amp_to_db = T.AmplitudeToDB(top_db=80)

    spec = transform(audio_chunk)
    log_spec = amp_to_db(spec)
    log_spec = log_spec.permute(0, 2, 1)

    if log_spec.shape[1] > 96:
        log_spec = log_spec[:, :96, :]
    elif log_spec.shape[1] < 96:
        pad_h = 96 - log_spec.shape[1]
        log_spec = torch.nn.functional.pad(log_spec, (0, 0, 0, pad_h))

    return log_spec.unsqueeze(0)

def load_audio_model(model_path, device='cuda'):
    base_dir = os.path.dirname(os.path.abspath(__file__))
    repo_path = os.path.join(base_dir, 'torch_yamnet')
    
    if repo_path not in sys.path:
        sys.path.append(repo_path)

    try:
        from torch_audioset.yamnet.model import yamnet
        model = yamnet(pretrained=False)
    except ImportError as e:
        print(f"âŒ Import Failed: {e}")
        return None

    model.classifier = torch.nn.Linear(1024, 3)
    
    try:
        if os.path.exists(model_path):
            state_dict = torch.load(model_path, map_location=device)
            model.load_state_dict(state_dict)
            print(f"   âœ… Audio Weights loaded: {model_path}")
        else:
            print(f"âŒ Weights file not found: {model_path}")
            return None
    except Exception as e:
        print(f"âŒ State Dict Load Error: {e}")
        return None
        
    model.to(device)
    model.eval()
    return model


# --- 2. ì˜¤ë””ì˜¤ ì¶”ë¡  ì—”ì§„ (ë°°ì¹˜ ì²˜ë¦¬ êµ¬ì¡°ë¡œ ë³€ê²½) ---
class AudioInferenceEngine:
    def __init__(self, model, device, login_id):
        self.model = model
        self.device = device
        self.login_id = login_id
        
        self.is_recording = False
        self.start_event_class = None
        self.silence_counter = 0         
        
        self.audio_buffer = []           
        self.session_start_time = None   
        
        self.session_timeline = []

        self.CLASS_NOISE = 0
        self.CLASS_SNORE = 1
        self.CLASS_BRUXISM = 2

    def process_chunk(self, audio_float, audio_bytes):
        # 1. ì¶”ë¡ 
        input_tensor = preprocess_audio_chunk(audio_float).to(self.device)
        with torch.no_grad():
            raw_output = self.model(input_tensor)
            if isinstance(raw_output, (tuple, list)):
                outputs = raw_output[0]
            else:
                outputs = raw_output

            probs = torch.nn.functional.softmax(outputs, dim=1)
            confidence, predicted = torch.max(probs, 1)
            label = predicted.item()
            conf = confidence.item()

        # 2. ìƒíƒœ ë¨¸ì‹  ë¡œì§
        if not self.is_recording:
            # [IDLE] -> [START]
            if label in [self.CLASS_SNORE, self.CLASS_BRUXISM] and conf >= CONF_THRESHOLD:
                print(f"ğŸ”Š [START] Audio Event Detected: Class {label} (Conf: {conf:.2f})")
                self._start_session(label, audio_bytes)
        
        else:
            # [RECORDING]
            self.audio_buffer.append(audio_bytes)

            # ì—„ê²©í•œ í´ë˜ìŠ¤ ìœ ì§€ ë¡œì§
            if label == self.start_event_class:
                self.silence_counter = 0 
            else:
                self.silence_counter += 1 

            # ì¢…ë£Œ ì¡°ê±´ ì²´í¬ (10ì´ˆ ì´ìƒ ë³¸ë˜ ì´ë²¤íŠ¸ ë¯¸ê°ì§€)
            if self.silence_counter >= SILENCE_TIMEOUT:
                # ì—¬ê¸°ì„œ _end_sessionì„ í˜¸ì¶œí•˜ë©´ í›„ì²˜ë¦¬(Trimming)ê°€ ì§„í–‰ë¨
                print(f"â¹ [END] Silence Timeout Reached. Trimming last {self.silence_counter}s...")
                self._end_session()

    def _start_session(self, label, first_chunk):
        self.is_recording = True
        self.start_event_class = label
        self.silence_counter = 0
        self.session_start_time = datetime.now()
        self.audio_buffer = [first_chunk]

    def _end_session(self):
        """ì„¸ì…˜ ì¢…ë£Œ: ë’·ë¶€ë¶„(ì¹¨ë¬µ/Noise êµ¬ê°„)ì„ ì˜ë¼ë‚´ê³  ì €ì¥"""
        now_time = datetime.now()
        trim_seconds = self.silence_counter # ì˜ë¼ë‚´ì•¼ í•  ì‹œê°„ (ì´ˆ)

        # 1. ì‹¤ì œ ì¢…ë£Œ ì‹œê°„ ë³´ì • (í˜„ì¬ ì‹œê°„ - ê¸°ë‹¤ë¦° ì‹œê°„)
        real_end_time = now_time - timedelta(seconds=trim_seconds)
        
        # 2. ì˜¤ë””ì˜¤ ë²„í¼ ìŠ¬ë¼ì´ì‹± (Trimming)
        # buffer[:-0]ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ë˜ë¯€ë¡œ trim_seconds > 0ì¼ ë•Œë§Œ ì²˜ë¦¬
        if trim_seconds > 0:
            # ë§ˆì§€ë§‰ nì´ˆ ë°ì´í„°ë¥¼ ë²„ë¦¼
            final_audio_data = self.audio_buffer[:-trim_seconds]
        else:
            # ê°•ì œ ì¢…ë£Œ ë“±ìœ¼ë¡œ ì¸í•´ ì¹´ìš´í„°ê°€ 0ì´ë©´ ê·¸ëŒ€ë¡œ ì €ì¥
            final_audio_data = self.audio_buffer

        # í˜¹ì‹œë¼ë„ ë²„í¼ê°€ ë¹„ì–´ë²„ë¦¬ë©´(ì´ë²¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•˜ì„ ê²½ìš°) ìµœì†Œ 1ì´ˆëŠ” ìœ ì§€
        if not final_audio_data and self.audio_buffer:
             final_audio_data = self.audio_buffer[:1]

        # 3. íŒŒì¼ ì €ì¥ (ì˜ë¦° ë°ì´í„°ë¡œ ì €ì¥)
        timestamp = self.session_start_time.strftime("%Y%m%d_%H%M%S")
        filename = f"{self.login_id}_{timestamp}_{self.start_event_class}.wav"
        full_filepath = os.path.join(RECORDING_DIR, filename)
        
        self._save_wav(full_filepath, final_audio_data)
        
        # ì‹¤ì œ ì €ì¥ëœ ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚° (ë¡œê·¸ìš©)
        duration_sec = len(final_audio_data)
        print(f"   âœ‚ï¸ Trimmed: {trim_seconds}s removed. Final Duration: {duration_sec}s")

        # 4. íƒ€ì„ë¼ì¸ ì¶”ê°€ (DB ì €ì¥ìš©)
        # DBì—ë„ 'ê°ì§€ ì¢…ë£Œ í›„ 10ì´ˆ ë’¤'ê°€ ì•„ë‹Œ 'ì‹¤ì œ ì†Œë¦¬ê°€ ëë‚œ ì‹œê°„'ì„ ê¸°ë¡
        self.session_timeline.append({
            'class': self.start_event_class,
            'start': self.session_start_time,
            'end': real_end_time,  # ë³´ì •ëœ ì¢…ë£Œ ì‹œê°„
            'path': full_filepath
        })

        # ì´ˆê¸°í™”
        self.is_recording = False
        self.audio_buffer = []
        self.start_event_class = None
        self.silence_counter = 0

    def _save_wav(self, filepath, buffer_list):
        try:
            with wave.open(filepath, 'wb') as wf:
                wf.setnchannels(1) 
                wf.setsampwidth(2) 
                wf.setframerate(SAMPLE_RATE)
                wf.writeframes(b''.join(buffer_list))
            print(f"   ğŸ’¾ Saved recording: {filepath}")
        except Exception as e:
            print(f"   âŒ Failed to save wav: {e}")

    def force_close(self):
        if self.is_recording:
            print("âš ï¸ Force closing active audio session...")
            # ê°•ì œ ì¢…ë£Œ ì‹œì—ëŠ” Trimmingì„ í• ì§€ ë§ì§€ ê²°ì •í•´ì•¼ í•¨
            # ë³´í†µ ê°•ì œ ì¢…ë£ŒëŠ” ì‚¬ìš©ìê°€ ëˆ ê²ƒì´ë¯€ë¡œ í˜„ì¬ê¹Œì§€ ë…¹ìŒëœê±¸ ë‹¤ ì €ì¥í•˜ëŠ”ê²Œ ì•ˆì „
            self.silence_counter = 0 
            self._end_session()
        return self.session_timeline


# --- 3. DB ì¼ê´„ ì €ì¥ í•¨ìˆ˜ ---
def save_audio_to_mariadb(login_id, session_data_list):
    """ëª¨ì•„ë‘” ì˜¤ë””ì˜¤ ì„¸ì…˜ ì •ë³´ë¥¼ DBì— í•œ ë²ˆì— ì €ì¥"""
    if not session_data_list:
        print("âš ï¸ [Audio] ì €ì¥í•  ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nğŸ’¾ [DB ì €ì¥] ìœ ì € {login_id} ì˜¤ë””ì˜¤ ê¸°ë¡ {len(session_data_list)}ê±´ ì €ì¥ ì‹œì‘")

    conn = get_db_connection()
    if conn is None:
        print("âŒ DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì˜¤ë””ì˜¤ ë°ì´í„° ì €ì¥ ì¤‘ë‹¨")
        return

    try:
        with conn.cursor() as cur:
            # sleep_audio_log ì»¬ëŸ¼ì— íŒŒì¼ ê²½ë¡œ(data['path'])ë¥¼ ë„£ìŠµë‹ˆë‹¤.
            insert_sql = """
            INSERT INTO sleep_audio (user_id, audio_class, st_dt, ed_dt, sleep_audio_log, dt)
            VALUES (%s, %s, %s, %s, %s, %s)
            """

            rows = []
            now_dt = datetime.now()
            for data in session_data_list:
                rows.append((
                    login_id,
                    data['class'],
                    data['start'],
                    data['end'],
                    data['path'], # íŒŒì¼ ê²½ë¡œ ë¬¸ìì—´
                    now_dt
                ))

            cur.executemany(insert_sql, rows)
            conn.commit()
            print(f"âœ… ì˜¤ë””ì˜¤ DB ì €ì¥ ì™„ë£Œ ({len(rows)}ê±´)")

    except Exception as e:
        conn.rollback()
        print("âŒ ì˜¤ë””ì˜¤ DB ì €ì¥ ì‹¤íŒ¨:", e)
    finally:
        conn.close()


# --- 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def run_audio_inference(source, stop_flag, login_id, model_path="yamnet_finetuned_best.pth"):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ¤ Audio Inference Started on {device} (Mode: Batch Save)")

    try:
        model = load_audio_model(model_path, device)
        if model is None: return
        engine = AudioInferenceEngine(model, device, login_id)
    except Exception as e:
        print(f"âŒ Audio Init Error: {e}")
        return

    # FFmpeg ì„¤ì •
    if os.path.isfile(source):
        cmd = ["ffmpeg", "-loglevel", "quiet", "-i", source, "-ac", "1", "-ar", str(SAMPLE_RATE), "-f", "s16le", "-"]
    else:
        cmd = ["ffmpeg", "-loglevel", "quiet", "-rtsp_transport", "tcp", "-i", source, "-ac", "1", "-ar", str(SAMPLE_RATE), "-f", "s16le", "-"]
    
    process = None
    try:
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
        read_chunk_size = CHUNK_SIZE

        while not stop_flag():
            audio_bytes = process.stdout.read(read_chunk_size)
            if not audio_bytes: break
            
            if len(audio_bytes) < read_chunk_size:
                audio_bytes += b'\x00' * (read_chunk_size - len(audio_bytes))

            audio_int16 = np.frombuffer(audio_bytes, dtype=np.int16)
            audio_float = audio_int16.astype(np.float32) / 32768.0
            
            engine.process_chunk(audio_float, audio_bytes)
            
    except Exception as e:
        print(f"Audio Loop Error: {e}")
        
    finally:
        # 1. FFmpeg ì¢…ë£Œ
        if process: process.terminate()
        
        # 2. ì§„í–‰ ì¤‘ì´ë˜ ì„¸ì…˜ ê°•ì œ ì¢…ë£Œ ë° ë°ì´í„° í™•ë³´
        final_timeline = engine.force_close()
        
        # 3. [NEW] DB ì¼ê´„ ì €ì¥
        if final_timeline:
            save_audio_to_mariadb(login_id, final_timeline)
            
        print("ğŸ¤ Audio Inference Stopped Completely")