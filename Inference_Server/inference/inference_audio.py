import sys
import os
import torch
import torchaudio.transforms as T
import numpy as np
import subprocess
import wave
from datetime import datetime, timedelta
from Inference_Server.inference.db_utils import get_db_connection

# --- ì„¤ì •ê°’ ---
SAMPLE_RATE = 16000
CHUNK_DURATION = 1  # 1ì´ˆ
CHUNK_SIZE = SAMPLE_RATE * CHUNK_DURATION * 2  # 16bit = 2bytes -> 32000 bytes
CONF_THRESHOLD = 0.85  # ì‹œì‘ ì„ê³„ê°’ (60%)
SILENCE_TIMEOUT = 10  # ë‹¤ë¥¸ ì´ë²¤íŠ¸ë‚˜ ì†ŒìŒì´ ì§€ì†ë˜ë©´ ì¢…ë£Œí•  ì‹œê°„ (ì´ˆ)
RECORDING_DIR = "./recordings"  # ë…¹ìŒ íŒŒì¼ ì €ì¥ ê²½ë¡œ

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(RECORDING_DIR, exist_ok=True)

# --- 1. ì „ì²˜ë¦¬ ë° ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼) ---
def rms_normalize(audio_chunk, target_rms=0.1):
    rms = np.sqrt(np.mean(audio_chunk**2))
    if rms < 1e-6:
        return audio_chunk
    gain = target_rms / rms
    return audio_chunk * gain

def preprocess_audio_chunk(audio_chunk):
    audio_chunk = rms_normalize(audio_chunk)
    if isinstance(audio_chunk, np.ndarray):
        audio_chunk = torch.from_numpy(audio_chunk).float()
    if len(audio_chunk.shape) == 1:
        audio_chunk = audio_chunk.unsqueeze(0)

    transform = T.MelSpectrogram(
        sample_rate=SAMPLE_RATE, n_fft=512, win_length=400, hop_length=160,
        n_mels=64, f_min=125, f_max=7500, center=False
    )
    amp_to_db = T.AmplitudeToDB(top_db=80)

    spec = transform(audio_chunk)
    log_spec = amp_to_db(spec)
    log_spec = log_spec.permute(0, 2, 1)

    if log_spec.shape[1] > 96:
        log_spec = log_spec[:, :96, :]
    elif log_spec.shape[1] < 96:
        pad_h = 96 - log_spec.shape[1]
        log_spec = torch.nn.functional.pad(log_spec, (0, 0, 0, pad_h))

    return log_spec.unsqueeze(0)

def load_audio_model(model_path, device='cuda'):
    base_dir = os.path.dirname(os.path.abspath(__file__))
    repo_path = os.path.join(base_dir, 'torch_yamnet')
    
    if repo_path not in sys.path:
        sys.path.append(repo_path)

    try:
        from torch_audioset.yamnet.model import yamnet
        model = yamnet(pretrained=False)
    except ImportError as e:
        print(f"âŒ Import Failed: {e}")
        return None

    model.classifier = torch.nn.Linear(1024, 3)
    
    try:
        if os.path.exists(model_path):
            state_dict = torch.load(model_path, map_location=device)
            model.load_state_dict(state_dict)
            print(f"   âœ… Audio Weights loaded: {model_path}")
        else:
            print(f"âŒ Weights file not found: {model_path}")
            return None
    except Exception as e:
        print(f"âŒ State Dict Load Error: {e}")
        return None
        
    model.to(device)
    model.eval()
    return model


# --- 2. ì˜¤ë””ì˜¤ ì¶”ë¡  ì—”ì§„ (ì‹œê°„ ê³„ì‚° ë¡œì§ ìˆ˜ì •) ---
class AudioInferenceEngine:
    def __init__(self, model, device, login_id):
        self.model = model
        self.device = device
        self.login_id = login_id
        
        self.is_recording = False
        self.start_event_class = None
        self.silence_counter = 0         
        
        self.audio_buffer = []           
        
        # [ìˆ˜ì •] ì‹œê°„ ê´€ë¦¬ë¥¼ ìœ„í•œ ë³€ìˆ˜
        self.base_timestamp = datetime.now() # ì—”ì§„ ì‹œì‘ ì‹œê° (ê¸°ì¤€ì )
        self.processed_seconds = 0.0         # í˜„ì¬ê¹Œì§€ ì²˜ë¦¬í•œ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
        self.current_start_offset = 0.0      # í˜„ì¬ ì„¸ì…˜ì´ ì‹œì‘ëœ ì˜¤ë””ì˜¤ ì‹œì  (ì´ˆ)
        
        self.session_timeline = []

        self.CLASS_NOISE = 0
        self.CLASS_SNORE = 1
        self.CLASS_BRUXISM = 2

    def process_chunk(self, audio_float, audio_bytes):
        # 1. ì¶”ë¡ 
        input_tensor = preprocess_audio_chunk(audio_float).to(self.device)
        with torch.no_grad():
            raw_output = self.model(input_tensor)
            if isinstance(raw_output, (tuple, list)):
                outputs = raw_output[0]
            else:
                outputs = raw_output

            probs = torch.nn.functional.softmax(outputs, dim=1)
            confidence, predicted = torch.max(probs, 1)
            label = predicted.item()
            conf = confidence.item()

        # 2. ìƒíƒœ ë¨¸ì‹  ë¡œì§
        if not self.is_recording:
            # [IDLE] -> [START]
            if label in [self.CLASS_SNORE, self.CLASS_BRUXISM] and conf >= CONF_THRESHOLD:
                print(f"ğŸ”Š [START] Audio Event Detected: Class {label} (Conf: {conf:.2f})")
                self._start_session(label, audio_bytes)
        
        else:
            # [RECORDING]
            self.audio_buffer.append(audio_bytes)

            if label == self.start_event_class:
                self.silence_counter = 0 
            else:
                self.silence_counter += 1 

            # ì¢…ë£Œ ì¡°ê±´ ì²´í¬
            if self.silence_counter >= SILENCE_TIMEOUT:
                print(f"â¹ [END] Silence Timeout Reached. Trimming last {self.silence_counter}s...")
                self._end_session()

        # [ì¤‘ìš”] ì²­í¬ ì²˜ë¦¬ê°€ ëë‚  ë•Œë§ˆë‹¤ ì˜¤ë””ì˜¤ ì‹œê°„ ëˆ„ì  (í•­ìƒ 1ì´ˆì”© ì¦ê°€)
        self.processed_seconds += CHUNK_DURATION

    def _start_session(self, label, first_chunk):
        self.is_recording = True
        self.start_event_class = label
        self.silence_counter = 0
        self.audio_buffer = [first_chunk]
        
        # [ìˆ˜ì •] ì‹œì‘ ì‹œê°„ = ê¸°ì¤€ ì‹œê°„ + í˜„ì¬ê¹Œì§€ íë¥¸ ì˜¤ë””ì˜¤ ì‹œê°„
        # datetime.now()ë¥¼ ì“°ì§€ ì•Šì•„ íŒŒì¼ ì²˜ë¦¬ ì†ë„ì™€ ë¬´ê´€í•˜ê²Œ ì •í™•í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°
        self.current_start_offset = self.processed_seconds

    def _end_session(self):
        trim_seconds = self.silence_counter 

        # [ìˆ˜ì •] ì¢…ë£Œ ì‹œê°„ ê³„ì‚°
        # ì¢…ë£Œ ì‹œì  ì˜¤ë””ì˜¤ ì‹œê°„ = í˜„ì¬ ëˆ„ì  ì˜¤ë””ì˜¤ ì‹œê°„ - Trim ì‹œê°„
        real_end_offset = self.processed_seconds - trim_seconds
        
        # 1. ì˜¤ë””ì˜¤ ë²„í¼ Trimming
        if trim_seconds > 0:
            final_audio_data = self.audio_buffer[:-trim_seconds]
        else:
            final_audio_data = self.audio_buffer

        if not final_audio_data and self.audio_buffer:
             final_audio_data = self.audio_buffer[:1]

        # 2. Datetime ë³€í™˜ (DB ì €ì¥ìš©)
        # ê¸°ì¤€ ì‹œê°ì— ì˜¤í”„ì…‹(ì´ˆ)ì„ ë”í•´ì„œ ìµœì¢… ì‹œê°„ ê³„ì‚°
        st_dt = self.base_timestamp + timedelta(seconds=self.current_start_offset)
        ed_dt = self.base_timestamp + timedelta(seconds=real_end_offset)

        # 3. íŒŒì¼ ì €ì¥
        timestamp = st_dt.strftime("%Y%m%d_%H%M%S")
        filename = f"{self.login_id}_{timestamp}_{self.start_event_class}.wav"
        full_filepath = os.path.join(RECORDING_DIR, filename)
        
        self._save_wav(full_filepath, final_audio_data)
        
        duration_sec = len(final_audio_data)
        print(f"   âœ‚ï¸ Trimmed: {trim_seconds}s removed. Final Duration: {duration_sec}s")
        print(f"   ğŸ•’ Time: {st_dt} ~ {ed_dt}")

        # 4. íƒ€ì„ë¼ì¸ ì¶”ê°€
        self.session_timeline.append({
            'class': self.start_event_class,
            'start': st_dt, # ê³„ì‚°ëœ ì‹œì‘ ì‹œê°„
            'end': ed_dt,   # ê³„ì‚°ëœ ì¢…ë£Œ ì‹œê°„ (ë¬´ì¡°ê±´ startë³´ë‹¤ ë’¤ì„)
            'path': full_filepath
        })

        # ì´ˆê¸°í™”
        self.is_recording = False
        self.audio_buffer = []
        self.start_event_class = None
        self.silence_counter = 0

    def _save_wav(self, filepath, buffer_list):
        try:
            with wave.open(filepath, 'wb') as wf:
                wf.setnchannels(1) 
                wf.setsampwidth(2) 
                wf.setframerate(SAMPLE_RATE)
                wf.writeframes(b''.join(buffer_list))
            print(f"   ğŸ’¾ Saved recording: {filepath}")
        except Exception as e:
            print(f"   âŒ Failed to save wav: {e}")

    def force_close(self):
        if self.is_recording:
            print("âš ï¸ Force closing active audio session...")
            self.silence_counter = 0 
            self._end_session()
        return self.session_timeline


# --- 3. DB ì¼ê´„ ì €ì¥ í•¨ìˆ˜ ---
def save_audio_to_mariadb(login_id, session_data_list):
    """ëª¨ì•„ë‘” ì˜¤ë””ì˜¤ ì„¸ì…˜ ì •ë³´ë¥¼ DBì— í•œ ë²ˆì— ì €ì¥"""
    if not session_data_list:
        print("âš ï¸ [Audio] ì €ì¥í•  ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nğŸ’¾ [DB ì €ì¥] ìœ ì € {login_id} ì˜¤ë””ì˜¤ ê¸°ë¡ {len(session_data_list)}ê±´ ì €ì¥ ì‹œì‘")

    conn = get_db_connection()
    if conn is None:
        print("âŒ DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì˜¤ë””ì˜¤ ë°ì´í„° ì €ì¥ ì¤‘ë‹¨")
        return

    try:
        with conn.cursor() as cur:
            # sleep_audio_log ì»¬ëŸ¼ì— íŒŒì¼ ê²½ë¡œ(data['path'])ë¥¼ ë„£ìŠµë‹ˆë‹¤.
            insert_sql = """
            INSERT INTO sleep_audio (user_id, audio_class, st_dt, ed_dt, sleep_audio_log, dt)
            VALUES (%s, %s, %s, %s, %s, %s)
            """

            rows = []
            now_dt = datetime.now()
            for data in session_data_list:
                rows.append((
                    login_id,
                    data['class'],
                    data['start'],
                    data['end'],
                    data['path'], # íŒŒì¼ ê²½ë¡œ ë¬¸ìì—´
                    now_dt
                ))

            cur.executemany(insert_sql, rows)
            conn.commit()
            print(f"âœ… ì˜¤ë””ì˜¤ DB ì €ì¥ ì™„ë£Œ ({len(rows)}ê±´)")

    except Exception as e:
        conn.rollback()
        print("âŒ ì˜¤ë””ì˜¤ DB ì €ì¥ ì‹¤íŒ¨:", e)
    finally:
        conn.close()


# --- 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def run_audio_inference(source, stop_flag, login_id, model_path="yamnet_finetuned_best.pth"):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ¤ Audio Inference Started on {device} (Mode: Batch Save)")

    try:
        model = load_audio_model(model_path, device)
        if model is None: return
        engine = AudioInferenceEngine(model, device, login_id)
    except Exception as e:
        print(f"âŒ Audio Init Error: {e}")
        return

    # FFmpeg ì„¤ì •
    if os.path.isfile(source):
        cmd = ["ffmpeg", "-loglevel", "quiet", "-i", source, "-ac", "1", "-ar", str(SAMPLE_RATE), "-f", "s16le", "-"]
    else:
        cmd = ["ffmpeg", "-loglevel", "quiet", "-rtsp_transport", "tcp", "-i", source, "-ac", "1", "-ar", str(SAMPLE_RATE), "-f", "s16le", "-"]
    
    process = None
    try:
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
        read_chunk_size = CHUNK_SIZE

        while not stop_flag():
            audio_bytes = process.stdout.read(read_chunk_size)
            if not audio_bytes: break
            
            if len(audio_bytes) < read_chunk_size:
                audio_bytes += b'\x00' * (read_chunk_size - len(audio_bytes))

            audio_int16 = np.frombuffer(audio_bytes, dtype=np.int16)
            audio_float = audio_int16.astype(np.float32) / 32768.0
            
            engine.process_chunk(audio_float, audio_bytes)
            
    except Exception as e:
        print(f"Audio Loop Error: {e}")
        
    finally:
        # 1. FFmpeg ì¢…ë£Œ
        if process: process.terminate()
        
        # 2. ì§„í–‰ ì¤‘ì´ë˜ ì„¸ì…˜ ê°•ì œ ì¢…ë£Œ ë° ë°ì´í„° í™•ë³´
        final_timeline = engine.force_close()
        
        # 3. [NEW] DB ì¼ê´„ ì €ì¥
        if final_timeline:
            save_audio_to_mariadb(login_id, final_timeline)
            
        print("ğŸ¤ Audio Inference Stopped Completely")