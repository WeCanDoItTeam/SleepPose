from ultralytics import YOLO
import os
import sys
import cv2
import numpy as np
import timm
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score
import seaborn as sns
from pathlib import Path
from tabulate import tabulate

# =========================================================
# Utils
# =========================================================

KPT_ALPHA = 0.85
# ì ì™¸ì„  í™˜ê²½ ë” ì˜ ë³´ì´ê²Œ ì²˜ë¦¬
def ir_preprocess(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(2.0, (8,8))
    gray = clahe.apply(gray)
    return cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)

# ìŠ¤ë¬´ìŠ¤í•˜ê²Œ í‚¤í¬ì¸íŠ¸ ì´ë™
def ema(prev, curr, alpha):
    return curr if prev is None else alpha * prev + (1 - alpha) * curr

# ê¸‰ê²©í•œ ì¢Œìš° ë’¤ì§‘í˜ ë°©ì§€
def enforce_lr_consistency(kpts):
    """ì¢Œ/ìš° ìŠ¤ì™‘ ë°©ì§€"""
    pairs = [(5,6),(7,8),(9,10),(11,12),(13,14),(15,16)]
    if np.isnan(kpts[5]).any() or np.isnan(kpts[6]).any():
        return kpts

    if kpts[5,0] > kpts[6,0]:
        for a,b in pairs:
            kpts[[a,b]] = kpts[[b,a]]
    return kpts

# ê¸°ì¡´ ë¼ë²¨ì„ (ë°”ìš´ë”© ë°•ìŠ¤(ì›ë³¸ ì´ë¯¸ì§€ í”½ì…€) / ë°”ìš´ë”© ë°•ìŠ¤(ì›ë³¸ ì´ë¯¸ì§€ ì •ê·œí™”) + í‚¤í¬ì¸íŠ¸(ì´ë¯¸ ì •ê·œí™”) / í´ë˜ìŠ¤ ì•„ì´ë””)ë¡œ ë¶„ë¥˜
def load_yolo_pose_label(label_path, img_w, img_h):

    data = np.loadtxt(label_path).reshape(-1)

    if data.shape[0] < 5 + 17 * 3:
        raise ValueError(f"Invalid label format: {label_path}")

    # Class & BBox (normalized)
    cls = int(data[0])
    xc, yc, w, h = data[1:5]

    # BBox (pixel coords)
    x1 = int((xc - w / 2) * img_w)
    y1 = int((yc - h / 2) * img_h)
    x2 = int((xc + w / 2) * img_w)
    y2 = int((yc + h / 2) * img_h)

    # Keypoints (already normalized)
    kpts = data[5:5 + 17 * 3].reshape(17, 3).astype(np.float32)

    # ğŸ”’ ì•ˆì „ ì²˜ë¦¬: ì¢Œí‘œ í´ë¦¬í•‘
    kpts[:, 0] = np.clip(kpts[:, 0], 0.0, 1.0)
    kpts[:, 1] = np.clip(kpts[:, 1], 0.0, 1.0)

    # bbox(ì •ê·œí™” ëœ ìƒíƒœ) + kps => MLPìš© ë°ì´í„°
    pose_feature = np.concatenate([
    np.array([xc, yc, w, h], dtype=np.float32), # (4,)
    kpts.flatten()  # (51,)
    ])  # â†’ (55,)

    return (x1, y1, x2, y2), pose_feature, cls

# ì´ë¯¸ì§€ í¬ë¡­
def crop_image(img, bbox):
    x1, y1, x2, y2 = bbox
    h, w = img.shape[:2]
    x1, y1 = max(0, x1), max(0, y1)
    x2, y2 = min(w, x2), min(h, y2)

    if x2 <= x1 or y2 <= y1:
        # print(img_path)
        raise ValueError("Invalid crop region")

    crop = img[y1:y2, x1:x2]
    crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)

    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    return transform(crop_rgb)

# í¬ë¡­ëœ ì´ë¯¸ì§€ ê¸°ì¤€ í‚¤í¬ì¸íŠ¸ ë³€ê²½
def normalize_kpts_to_crop(kpts, bbox, img_w, img_h):
    x1, y1, x2, y2 = bbox
    bw = max(x2 - x1, 1e-6)
    bh = max(y2 - y1, 1e-6)

    kpts_crop = []
    kpts_only = kpts[4:].reshape(17,3).astype(np.float32)

    for x, y, c in kpts_only:
        if c == 0:
            kpts_crop.append([0.0, 0.0, 0])
            continue

        # âœ… ì›ë³¸ ì´ë¯¸ì§€ ê¸°ì¤€ í”½ì…€ ì¢Œí‘œ
        px = x * img_w
        py = y * img_h

        # crop ê¸°ì¤€ ì •ê·œí™”
        cx = (px - x1) / bw
        cy = (py - y1) / bh

        kpts_crop.append([
            np.clip(cx, 0.0, 1.0),
            np.clip(cy, 0.0, 1.0),
            c
        ])

    kpts_crop = np.array(kpts_crop, dtype=np.float32).flatten()

    return np.concatenate([kpts[:4], kpts_crop])

# =========================================================
# Dataset
# =========================================================

# trainì—ì„œë§Œ ì‚¬ìš©
class SleepPoseDataset(Dataset):
    def __init__(self, img_dir, label_dir):
        self.img_dir = img_dir
        self.label_dir = label_dir

        self.images = [f for f in os.listdir(img_dir) if f.lower().endswith('.jpg')]
        if not self.images:
            raise RuntimeError("No images found in dataset")

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        try:    
            img_name = self.images[idx]
            img_path = os.path.join(self.img_dir, img_name)
            # print(img_path)
            label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.txt'))
            img = cv2.imread(img_path)
            if img is None:
                raise self.__getitem__((idx + 1) % len(self))

            h, w = img.shape[:2]

            # ë¼ë²¨ì •ë³´ê°€ì ¸ì™€ì„œ ì´ë¯¸ì§€crop
            bbox, kpts, cls = load_yolo_pose_label(label_path, w, h)
            crop = crop_image(img, bbox)
            # í¬ë¡­ëœ ì´ë¯¸ì§€ë¡œ kpts ì¬ì¡°ì •
            kpts_norm = normalize_kpts_to_crop(kpts, bbox, w, h)

            return crop, torch.from_numpy(kpts_norm), torch.tensor(cls, dtype=torch.long)
        
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë‹¤ìŒ ì¸ë±ìŠ¤ë¡œ ë„˜ì–´ê°€ê¸°
            print(f"Skipping {self.images[idx]} due to error: {e}")
            return self.__getitem__((idx + 1) % len(self))

# =========================================================
# Model
# =========================================================

# CNN
class ImageEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)
        self.model = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=True)
        self.out_dim = 1280
        
        # ê°€ì¤‘ì¹˜ ë™ê²°
        for param in self.model.parameters():
            param.requires_grad = False

        # top ë ˆì´ì–´ë§Œ í•™ìŠµ
        for name, param in self.model.named_parameters():
            if "blocks.4" in name or "blocks.5" in name:
                param.requires_grad = True

    def forward(self, x):
        x = self.model.forward_features(x) # (Batch, 1280, 7, 7) í˜•íƒœ
        x = torch.mean(x, dim=(2, 3), keepdim=True)
        return x.flatten(1)

# MLP
class KeypointEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(4 + 17 * 3, 128),
            nn.BatchNorm1d(128), # í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ ì¶”ê°€ ê¶Œì¥
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU()
        )
        self.out_dim = 512

    def forward(self, kpts):
        return self.net(kpts.flatten(1))

# ëª¨ë¸ ë³¸ì²´
class SleepPoseNet(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.img_enc = ImageEncoder()
        self.kpt_enc = KeypointEncoder()

        self.classifier = nn.Sequential(
            nn.Linear(self.img_enc.out_dim + self.kpt_enc.out_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, img, kpts):
        f_img = self.img_enc(img)
        f_kpt = self.kpt_enc(kpts)
        return self.classifier(torch.cat([f_img, f_kpt], dim=1))

from torch.nn import functional as F

# ë³€ê²½í•œ ì†ì‹¤í•¨ìˆ˜
class FocalLoss(nn.Module):
    def __init__(self, gamma=2, reduction='mean'):
        super().__init__()
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        loss = ((1 - pt) ** self.gamma) * ce_loss
        return loss.mean() if self.reduction=='mean' else loss.sum()

# =========================================================
# Training
# =========================================================

EPOCH = 25
BATCH_SIZE = 32

# í•™ìŠµ
def train():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # label, imageì½ì–´ì˜´, img crop
    dataset = SleepPoseDataset(r'C:\Users\USER\Documents\Github\SleepPose\Inference_Server\data\pose_Lee\train\images', 
                               r'C:\Users\USER\Documents\Github\SleepPose\Inference_Server\data\pose_Lee\train\labels')
    val_dataset = SleepPoseDataset(r'C:\Users\USER\Documents\Github\SleepPose\Inference_Server\data\pose_Lee\val\images', 
                                   r'C:\Users\USER\Documents\Github\SleepPose\Inference_Server\data\pose_Lee\val\labels')

    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    model = SleepPoseNet(num_classes=5).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    # class_weights = torch.tensor([
    #     1.0,  # lying
    #     1.0,  # side
    #     1.0,  # handup
    #     1.0,  # back
    #     1.0  # other
    # ], device=device)

    criterion = FocalLoss()
    # criterion = nn.CrossEntropyLoss(weight=class_weights)

    # ğŸ”¥ ìµœì  ëª¨ë¸ ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
    best_val_loss = float('inf')  # ì²˜ìŒì—ëŠ” ì•„ì£¼ í° ê°’ìœ¼ë¡œ ì„¤ì •
    save_path = 'sleep_pose_best_model.pt'

    train_losses = []
    val_losses = []
    val_accs = []
    best_cm = None

    best_metrics = {
    "val_loss": None,
    "val_acc": None,
    "precision": None,
    "recall": None,
    "f1": None
    }

    for epoch in range(EPOCH):
        model.train() # í•™ìŠµ ëª¨ë“œ   
        total_loss = 0.0
        for imgs, kpts, labels in loader:
            imgs, kpts, labels = imgs.to(device), kpts.to(device), labels.to(device)

            logits = model(imgs, kpts)
            loss = criterion(logits, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            train_loss = total_loss / len(loader)

        # ğŸ”´ VALIDATION (epoch ëë‚˜ê³  ë”± 1ë²ˆ)
        val_loss, val_acc, val_precision, val_recall, val_f1, cm = validate(model, val_loader, criterion, device)

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_accs.append(val_acc)

        print(
            f"Epoch {epoch + 1:02d} | "
            f"Train Loss: {train_loss:.4f} | "
            f"Val Loss: {val_loss:.4f} | "
            f"Val Acc: {val_acc:.4f}"
        )

        # Best model ì €ì¥ + best metrics ë³´ê´€
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_cm = cm

            best_metrics["val_loss"] = val_loss
            best_metrics["val_acc"] = val_acc
            best_metrics["precision"] = val_precision
            best_metrics["recall"] = val_recall
            best_metrics["f1"] = val_f1

            torch.save(model.state_dict(), save_path)
            print(f"âœ¨ Best model saved (Val Loss: {val_loss:.4f})")

    torch.save(model.state_dict(), 'sleep_pose_hybrid2_hj.pt')

    print("\n================ Final Metrics (Best Model) ================\n")

    table = [
        ["Train Loss (Last)", f"{train_losses[-1]:.4f}"],
        ["Best Val Loss", f"{best_metrics['val_loss']:.4f}"],
        ["Best Val Accuracy", f"{best_metrics['val_acc']:.4f}"],
        ["Precision (Macro)", f"{best_metrics['precision']:.4f}"],
        ["Recall (Macro)", f"{best_metrics['recall']:.4f}"],
        ["Macro F1-score", f"{best_metrics['f1']:.4f}"]
    ]

    print(tabulate(table, headers=["Metric", "Value"], tablefmt="grid"))

    # ============================
    # ğŸ“Š Loss & Accuracy Plot
    # ============================
    epochs = range(1, EPOCH + 1)

    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_losses, label="Train Loss")
    plt.plot(epochs, val_losses, label="Val Loss")
    plt.plot(epochs, val_accs, label="Val Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Value")
    plt.title("Training Curve")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("training_curve.jpg", dpi=300)
    plt.close()

    # ============================
    # ğŸ“Š í˜¼ë™ í–‰ë ¬ ì €ì¥
    # ============================
    class_names = ["lying", "side", "handup", "back", "other"]
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        best_cm,
        annot=True,
        fmt="d",
        cmap="Blues",
        xticklabels=class_names,
        yticklabels=class_names
    )

    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix (Best Val Loss Model)")
    plt.tight_layout()
    plt.savefig("confusion_matrix_best.jpg", dpi=300)
    plt.close()

# ---------------------------------------------------------
# Basic Inference Input Builder (IMAGE + YOLO OUTPUT)
# ---------------------------------------------------------

# ì¶”ë¡ ìš©: í¬ë¡­ëœ ì´ë¯¸ì§€ + (ë°”ìš´ë”© ë°•ìŠ¤ + í‚¤í¬ì¸íŠ¸)
def build_hybrid_inputs(image_bgr, bbox, bbox_n, kpts, device):
    # Crop person region
    crop = crop_image(image_bgr, bbox)

    img_tensor = crop.unsqueeze(0).to(device)

    kpts_flat = kpts.flatten() 

    kpts_add = np.concatenate([bbox_n, kpts_flat]) # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ê°€
    kpt_tensor = torch.from_numpy(kpts_add).unsqueeze(0).to(device) 

    return img_tensor, kpt_tensor

# ---------------------------------------------------------
# Final Prediction with Distinction
# ---------------------------------------------------------

# ì˜ˆì¸¡ ê²°ê³¼ í‚¤í¬ì¸íŠ¸ ì²˜ë¦¬
def predict_with_distinction(model, img, kpts, device, conf_thres=0.7):
    model.eval()
    with torch.no_grad():
        logits = model(img.to(device), kpts.to(device))
        probs = torch.softmax(logits, dim=1)[0]
        pred = int(torch.argmax(probs))
    
    if probs[pred] < conf_thres:
        pred = rule_based_postprocess(kpts)
        
    return pred # ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

# ë£° ê¸°ë°˜ ê²°ê³¼ ë³´ì •
def rule_based_postprocess(kpts, conf_thres=0.4, shoulder_parallel_deg=20):
    if isinstance(kpts, torch.Tensor):
        kpts = kpts.detach().cpu()

    # batch ì°¨ì› ì œê±°
    if kpts.ndim == 2:
        kpts = kpts[0]

    if kpts.numel() != 55:
        print(f"[DEBUG] invalid kpts numel = {kpts.numel()}")
        return 4

    kpts = kpts.numpy()

    # bbox ì œê±°
    kpts = kpts[4:].reshape(17, 3)

    # ì£¼ìš” í‚¤í¬ì¸íŠ¸
    nose = kpts[0]
    l_eye, r_eye = kpts[1], kpts[2]
    l_shoulder, r_shoulder = kpts[5], kpts[6]
    l_wrist, r_wrist = kpts[9], kpts[10]

    # =========================
    # STEP 1. ì•/ë’¤ íŒë³„
    # =========================
    face_conf_cnt = sum([
        nose[2] > conf_thres,
        l_eye[2] > conf_thres,
        r_eye[2] > conf_thres
    ])

    is_front = face_conf_cnt >= 2

    # =========================
    # STEP 2. ì•ì„ ë³´ê³  ìˆëŠ” ê²½ìš°
    # =========================
    if is_front:
        # ì–´ê¹¨ì„  ê¸°ìš¸ê¸°
        dx = r_shoulder[0] - l_shoulder[0]
        dy = r_shoulder[1] - l_shoulder[1]
        shoulder_angle = np.degrees(np.arctan2(dy, dx))

        is_parallel = abs(shoulder_angle) < shoulder_parallel_deg

        # ì†ëª© ìœ„ì¹˜
        wrist_up = (
            (l_wrist[2] > conf_thres and l_wrist[1] < l_shoulder[1]) or
            (r_wrist[2] > conf_thres and r_wrist[1] < r_shoulder[1])
        )

        if is_parallel:
            return 2 if wrist_up else 0
        else:
            return 1  # ì˜†ìœ¼ë¡œ ëˆ„ì›€

    # =========================
    # STEP 3. ì–¼êµ´ ì•ˆ ë³´ì„ â†’ ì—ë“œë¦¼
    # =========================
    return 3


# í•™ìŠµì—ì„œ ì“°ì´ëŠ” í‰ê°€ ë©”ì„œë“œ
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix

def validate(model, val_loader, criterion, device):
    model.eval()
    all_preds = []
    all_labels = []
    total_loss = 0.0

    with torch.no_grad():
        for imgs, kpts, labels in val_loader:
            imgs, kpts, labels = imgs.to(device), kpts.to(device), labels.to(device)
            logits = model(imgs, kpts)
            loss = criterion(logits, labels)
            total_loss += loss.item()

            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    val_loss = total_loss / len(val_loader)
    val_acc = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)
    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)
    cm = confusion_matrix(all_labels, all_preds)

    return val_loss, val_acc, precision, recall, f1, cm




# =========================================================
# Inference Example (YOLO-Pose â†’ Hybrid Prediction)
# =========================================================

# ì´ë¯¸ì§€ìš© ì¶”ë¡  ë©”ì„œë“œ

def predict_images(
    image_folder,
    yolo_weights="yolo11n-pose.pt",
    hybrid_weights="sleep_pose_hybrid_hj.pt",
    output_folder=None,
    conf_thres=0.3,
    stream=True
):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    hybrid_model = SleepPoseNet(num_classes=5).to(device)
    hybrid_model.load_state_dict(torch.load(hybrid_weights, map_location=device))
    hybrid_model.eval()

    yolo_model = YOLO(yolo_weights)

    if output_folder is not None:
        os.makedirs(output_folder, exist_ok=True)

    skeleton = [
        (5, 7), (7, 9),
        (6, 8), (8, 10),
        (5, 6),
        (5, 11), (6, 12),
        (11, 12),
        (11, 13), (13, 15),
        (12, 14), (14, 16)
    ]

    predictions = []

    img_paths = sorted(Path(image_folder).glob("*.[jp][pn]g"))  # jpg, png
    for img_path in img_paths:
        frame = cv2.imread(str(img_path))
        if frame is None:
            print(f"Failed to read image: {img_path}")
            predictions.append(None)
            continue

        cls_id = None
        try:
            results = yolo_model(frame, conf=conf_thres, iou=0.5, verbose=False)
            result = results[0]

            # ì‚¬ëŒì´ ê²€ì¶œë˜ì§€ ì•Šì€ ê²½ìš°
            if len(result.boxes) == 0 or result.keypoints is None:
                print(f"No person detected in {img_path.name}")
                predictions.append(None)
                if output_folder is not None:
                    out_path = os.path.join(output_folder, img_path.name)
                    cv2.imwrite(out_path, frame)
                continue

            # ì‚¬ëŒ ê²€ì¶œ ì‹œ ê¸°ì¡´ ì¶”ë¡  ë¡œì§
            bbox_xyxy = result.boxes.xyxy[0].cpu().numpy().astype(int)
            x1, y1, x2, y2 = bbox_xyxy
            bbox = tuple(bbox_xyxy)
            bbox_norm = result.boxes.xyxyn[0].cpu().numpy()

            kpts = result.keypoints.xy[0].cpu().numpy()
            kpts_norm = result.keypoints.xyn[0].cpu().numpy()
            kpts_conf = result.keypoints.conf[0].cpu().numpy().reshape(17, 1)
            kpts_n = np.concatenate([kpts_norm, kpts_conf], axis=1).astype(np.float32)

            img_t, kpt_t = build_hybrid_inputs(frame, bbox, bbox_norm, kpts_n, device)

            cls_id = predict_with_distinction(hybrid_model, img_t, kpt_t, device, conf_thres)
            print(f"{img_path.name}: class={cls_id}")

            # Draw bbox
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, f"class={cls_id}", (x1, max(0, y1-10)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

            restore_kpts_to_original(kpt_t, bbox, conf_thres)

            # Draw keypoints
            for i, (x, y) in enumerate(kpts):
                cv2.circle(frame, (int(x), int(y)), 3, (0, 0, 255), -1)

            # Draw skeleton
            for a, b in skeleton:
                x1_, y1_ = int(kpts[a][0]), int(kpts[a][1])
                x2_, y2_ = int(kpts[b][0]), int(kpts[b][1])
                cv2.line(frame, (x1_, y1_), (x2_, y2_), (255, 0, 0), 2)

            predictions.append(cls_id)

        except Exception as e:
            print(f"Error processing {img_path.name}: {e}")
            predictions.append(None)

        # ì €ì¥
        if output_folder is not None:
            out_path = os.path.join(output_folder, img_path.name)
            cv2.imwrite(out_path, frame)

        # í™”ë©´ ì¶œë ¥
        if stream:
            cv2.imshow("SleepPose Prediction", frame)
            if cv2.waitKey(1) & 0xFF == 27:
                break

    cv2.destroyAllWindows()
    return predictions


# ì•ˆ ì“°ì„
# def predictERror():

# =========================================================
# Minimal Self-Test (NO TRAINING)
# =========================================================

from ultralytics import YOLO

# ë¹„ë””ì˜¤ ì¶”ë¡ 
def predict_video(
    video_path,
    yolo_weights="yolo11n-pose.pt",
    hybrid_weights="sleep_pose_hybrid_hj.pt",
    output_path=None,
    conf_thres=0.7,
    stream=True
):

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    hybrid_model = SleepPoseNet(num_classes=5).to(device)
    hybrid_model.load_state_dict(torch.load(hybrid_weights, map_location=device))
    hybrid_model.eval()

    yolo_model = YOLO(yolo_weights)

    IMG_SIZE = 640

    prev_kpts_norm = None

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"Failed to open video: {video_path}")

    writer = None
    if output_path is not None:
        fps = cap.get(cv2.CAP_PROP_FPS)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(output_path, fourcc, fps, (IMG_SIZE, IMG_SIZE))

    predictions = []

    # COCO skeleton connections
    skeleton = [
        (5, 7), (7, 9),
        (6, 8), (8, 10),
        (5, 6),
        (5, 11), (6, 12),
        (11, 12),
        (11, 13), (13, 15),
        (12, 14), (14, 16)
    ]
    frame_count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        if frame_count % FRAME_SKIP != 0:
            # print(frame)
            continue

        frame = ir_preprocess(frame) # ì ì™¸ì„  í™˜ê²½ ì²˜ë¦¬
        cls_id = None
        results = yolo_model(frame, conf=CONF_THRES, iou=IOU_THRES, verbose=False)
        result = results[0] # ë¬´ì¡°ê±´ ì²« ë²ˆì§¸ ë°•ìŠ¤ë§Œ ê²€ì¶œ

        if len(result.boxes) > 0 and result.keypoints is not None:
            bbox_xyxy = result.boxes.xyxy[0].cpu().numpy().astype(int)
            x1, y1, x2, y2 = bbox_xyxy
            bbox = tuple(bbox_xyxy) # í”½ì…€ë‹¨ìœ„ (ì •ê·œí™” ì•„ë‹˜)
            bbox_norm = result.boxes.xyxyn[0].cpu().numpy() # í”½ì…€ë‹¨ìœ„ (ì •ê·œí™” ë¨)

            kpts = result.keypoints.xy[0].cpu().numpy() # í‚¤í¬ì¸íŠ¸ (ì •ê·œí™” ì•ˆ ë¨) cv ì¶œë ¥ìš©

            kpts_norm = result.keypoints.xyn[0].cpu().numpy() # í‚¤í¬ì¸íŠ¸ ì •ê·œí™”
            kpts_norm = enforce_lr_consistency(kpts_norm) # ê¸‰ê²©í•œ ë’¤ì§‘í˜ ë°©ì§€
            kpts_norm = ema(prev_kpts_norm, kpts_norm, KPT_ALPHA) # í‚¤í¬ì¸íŠ¸ ìŠ¤ë¬´ìŠ¤ ì´ë™
            prev_kpts_norm = kpts_norm.copy()
            kpts_conf = result.keypoints.conf[0].cpu().numpy().reshape(17, 1) # í‚¤í¬ì¸íŠ¸ ì‹ ë¢°ë„
            kpts_n = np.concatenate([kpts_norm, kpts_conf], axis=1).astype(np.float32)

            img_t, kpt_t = build_hybrid_inputs(frame, bbox, bbox_norm, kpts_n, device)


            # ëª¨ë¸ ì¶”ë¡ 
            cls_id = predict_with_distinction(
                hybrid_model, img_t, kpt_t, device, conf_thres
            )
            print(cls_id)
            # Draw bbox
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(
                frame,
                f"class={cls_id}",
                (x1, max(0, y1 - 10)),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.8,
                (0, 255, 0),
                2
            )

            # Draw keypoints
            for i, (x, y) in enumerate(kpts):
                cv2.circle(frame, (int(x), int(y)), 3, (0, 0, 255), -1)

            # Draw skeleton
            for a, b in skeleton:
                x1_, y1_ = int(kpts[a][0]), int(kpts[a][1])
                x2_, y2_ = int(kpts[b][0]), int(kpts[b][1])
                cv2.line(frame, (x1_, y1_), (x2_, y2_), (255, 0, 0), 2)

        predictions.append(cls_id)

        if writer is not None:
            writer.write(frame)

        if stream:
            cv2.imshow("SleepPose Prediction", frame)
            if cv2.waitKey(1) & 0xFF == 27:
                break

    cap.release()
    if writer is not None:
        writer.release()
    cv2.destroyAllWindows()

    return predictions

def restore_kpts_to_original(kpt_t, bbox, conf_thres=0.5):
    """
    kpt_t: (55,) torch or np  (bbox + 17 keypoints, normalized to crop)
    bbox: (x1, y1, x2, y2) in original pixel
    return: list of (x, y, conf) in original pixel or None
    """
    if isinstance(kpt_t, torch.Tensor):
        kpt_t = kpt_t.detach().cpu().numpy()

    kpt_t = kpt_t.reshape(-1)
    if kpt_t.size != 55:
        return []

    x1, y1, x2, y2 = bbox
    crop_w = max(x2 - x1, 1)
    crop_h = max(y2 - y1, 1)

    # bbox ì •ë³´ ì œê±° (ì• 4ê°œ)
    kpts = kpt_t[4:].reshape(17, 3)

    kpts_orig = []
    for x_n, y_n, c in kpts:
        if c < conf_thres:
            kpts_orig.append(None)
        else:
            x = int(x1 + x_n * crop_w)
            y = int(y1 + y_n * crop_h)
            kpts_orig.append((x, y, c))

    return kpts_orig

# ì¿ ë‹¤ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
def check_device():
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    cuda_available = torch.cuda.is_available()
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {cuda_available}")
    
    if cuda_available:
        # í˜„ì¬ ì„ íƒëœ GPU ì´ë¦„
        print(f"í˜„ì¬ GPU ì¥ì¹˜: {torch.cuda.get_device_name(0)}")
        # GPU ê°œìˆ˜
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        
        # ì‹¤ì œ í…ì„œë¥¼ ìƒì„±í•´ì„œ ì „ì†¡ í…ŒìŠ¤íŠ¸
        test_tensor = torch.zeros(1).to('cuda')
        print(f"í…ŒìŠ¤íŠ¸ í…ì„œ ìœ„ì¹˜: {test_tensor.device}")
    else:
        print("GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")

import matplotlib.pyplot as plt
import numpy as np

# í”„ë ˆì„ íƒ€ì„ê·¸ë˜í”„ ë³´ì—¬ì£¼ê¸°
def visualize_preds(preds, save_path="preds_timeline.jpg"):
    # None ì œê±° ë˜ëŠ” -1ë¡œ ì¹˜í™˜
    preds_clean = [-1 if p is None else p for p in preds]
    frames = np.arange(len(preds_clean))

    plt.figure(figsize=(15, 4))
    plt.plot(frames, preds_clean, marker='o', linestyle='-', alpha=0.7)

    plt.yticks(
        ticks=[-1, 0, 1, 2, 3, 4],
        labels=["None", "lying", "side", "handup", "back", "other"]
    )

    plt.xlabel("Frame Index")
    plt.ylabel("Predicted Class")
    plt.title("Sleep Pose Prediction Timeline")
    plt.grid(True, axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()

    print(f"ğŸ“Š Saved prediction timeline: {save_path}")

from collections import Counter

# í”„ë ˆì„ íšŸìˆ˜ ë³´ì—¬ì£¼ê¸°
def visualize_pred_distribution(preds, save_path="preds_distribution.jpg"):
    preds_clean = [p for p in preds if p is not None]
    counter = Counter(preds_clean)

    labels_map = {
        0: "lying",
        1: "side",
        2: "handup",
        3: "back",
        4: "other"
    }

    labels = [labels_map[k] for k in counter.keys()]
    values = list(counter.values())

    plt.figure(figsize=(6, 4))
    plt.bar(labels, values)
    plt.title("Sleep Pose Prediction Distribution")
    plt.ylabel("Frame Count")
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()

    print(f"ğŸ“Š Saved prediction distribution: {save_path}")

# ë¹„ìœ¨ ê³„ì‚°í•´ì„œ ë³´ì—¬ì£¼ê¸°
def visualize_pred_ratio(preds, fps=30, save_path="preds_ratio.jpg"):
    preds_clean = [p for p in preds if p is not None]
    counter = Counter(preds_clean)

    labels_map = {
        0: "lying",
        1: "side",
        2: "handup",
        3: "back",
        4: "other"
    }

    labels = []
    times = []

    for k, v in counter.items():
        labels.append(labels_map[k])
        times.append(v / fps / 60)  # ë¶„ ë‹¨ìœ„

    plt.figure(figsize=(6, 6))
    plt.pie(times, labels=labels, autopct="%.1f%%", startangle=90)
    plt.title("Sleep Pose Time Ratio (minutes)")
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()

    print(f"ğŸ›Œ Saved sleep pose ratio chart: {save_path}")

if __name__ == '__main__':

    FRAME_SKIP = 1
    CONF_THRES = 0.5
    IOU_THRES = 0.5

    name = "TEST_0.mp4"
    pt_name = "sleep_pose_best_model.pt"

    # í•™ìŠµì‹œí‚¤ê¸°
    # train()

    # ì‹¤í–‰
    # check_device()

    # preds = predict_images(
    #     image_folder=r"C:\Users\USER\Documents\Github\SleepPose\Inference_Server\data\pose_Lee\test\images",
    #     yolo_weights="yolo11n-pose.pt",
    #     hybrid_weights=rf"C:\Users\USER\Documents\Github\SleepPose\Inference_Server\pose_pt\pose_3_18e_rl1e-4_best\{pt_name}",
    #     output_folder=rf"C:\Users\USER\Documents\Github\SleepPose\Inference_Server\infer_images"
    # )
    # # í´ë˜ìŠ¤ ID â†’ í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘
    # class_names = {
    #     0: "ì •ìì„¸",
    #     1: "ì˜†ìœ¼ë¡œ ëˆ„ìš´ ìì„¸",
    #     2: "ì†ì„ ë“  ìì„¸",
    #     3: "ì—ë“œë¦° ìì„¸",
    #     4: "ê·¸ ì™¸ ìì„¸"
    # }

    # # predictions: ì´ë¯¸ì§€ë³„ ì¶”ë¡  ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    # # img_paths: ì´ë¯¸ì§€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (ê°™ì€ ìˆœì„œë¡œ ì •ë ¬ë˜ì–´ ìˆì–´ì•¼ í•¨)
    # img_paths = r"C:\Users\USER\Documents\Github\SleepPose\Inference_Server\data\pose_Lee\test\images"
    # for img_path, cls_id in zip(img_paths, preds):
    #     img_name = img_path.name
    #     if cls_id is None:
    #         print(f"{img_name}: ì‚¬ëŒ ì—†ìŒ")
    #     else:
    #         cls_name = class_names.get(cls_id, "ì•Œ ìˆ˜ ì—†ìŒ")
    #         print(f"{img_name}: {cls_name} (í´ë˜ìŠ¤ ID: {cls_id})")


    # predict video
    preds = predict_video(
        video_path=rf"C:\Users\USER\Documents\Github\SleepPose\Inference_Server\data\lee_video\infer_Oh.mp4",
        yolo_weights="yolo11n-pose.pt",
        hybrid_weights=rf"C:\Users\USER\Documents\Github\SleepPose\Inference_Server\pose_pt\pose_9_22e_rl1e-4_best\{pt_name}",
        #output_path=rf"C:\Users\USER\Documents\Github\SleepPose\Inference_Server\infer_video\{name}"
    )
    visualize_preds(preds, save_path="sleep_pose_timeline.jpg")
    visualize_pred_distribution(preds)
    visualize_pred_ratio(preds, fps=30)